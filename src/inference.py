import faulthandler
import pickle
import random
faulthandler.enable()

import warnings

import copy
import json
import math
import os
from functools import partial
import shutil
import sys
import time
import copy
import yaml
import numpy as np
import pandas as pd
from argparse import ArgumentParser, Namespace, FileType
from rdkit.Chem import RemoveHs
from rdkit import RDLogger
from rdkit.Chem import MolFromSmiles, AddHs
import torch
from torch_geometric.loader import DataLoader
from torch_geometric.nn.data_parallel import DataParallel
from rdkit import Chem

torch.multiprocessing.set_sharing_strategy('file_system')


from utils.diffusion_utils import t_to_sigma as t_to_sigma_compl
from utils.parsing import parse_train_args
from utils.training import inference_epoch, ListFilenameDataset, ListDataset
from utils.utils import save_yaml_file, get_optimizer_and_scheduler, get_model, ExponentialMovingAverage
from utils.diffusion_utils import t_to_sigma as t_to_sigma_compl, get_t_schedule
from utils.sampling import randomize_position, sampling_phore, calculate_fitscore
from datasets.pdbbind_phore import PhoreDataset, NoiseTransformPhore, construct_loader

RDLogger.DisableLog('rdApp.*')


def str2bool(inp):
    inp = inp.lower()
    if inp in ['y', 'yes', 'true', 't']:
        return True
    else:
        return False


def parse_args():
    parser = ArgumentParser()
    parser.add_argument('--config', type=FileType(mode='r'), default=None)

    # Input & Output
    parser.add_argument('--phore_ligand_csv', type=str, default=None, help='Path to a .csv file specifying the input. If this is not None, it will be used instead of the --protein and --ligand parameters')
    parser.add_argument('--phore', type=str, default=None, help='Path to the .phore file which is generated by AncPhore.')
    parser.add_argument('--ligand', type=str, default=None, help='Either a SMILES string or the path to a molecule file that rdkit can read.')
    parser.add_argument('--out_dir', type=str, default='results/user_inference', help='Directory where the outputs will be written to.')  
    parser.add_argument('--cache_path', type=str, default='data/cache', help='Folder from where to load/restore cached dataset')
    parser.add_argument('--split_file', type=str, default='data/splits/timesplit_no_lig_overlap_val', help='Select a dataset to test.')
    parser.add_argument('--overwrite', type=str2bool, default=False, help='Whether to store the ranked aligning poses.')
    parser.add_argument('--keep_local_structures', type=str2bool, default=False, help='Keeps the local structure when specifying an input with 3D coordinates instead of generating them with RDKit')
    parser.add_argument('--sample_per_complex', type=int, default=40, help='Samples of per complex.')
    parser.add_argument('--save_visualisation', action='store_true', default=False, help='Save a pdb file with all of the steps of the reverse diffusion')

    # Aligning Setting
    parser.add_argument('--model_dir', type=str, default='../results/0_diffphore_autow_dtn_phoreDedup_matchFeat_nv5_ft', help='Path to folder with trained score model and hyperparameters')
    parser.add_argument('--ckpt', type=str, default='best_ema_inference_epoch_model.pt', help='Checkpoint to use for the score model')
    parser.add_argument('--batch_size', type=int, default=32, help='')
    parser.add_argument('--num_workers', type=int, default=40, help='Number of workers for creating the dataset')
    parser.add_argument('--inference_steps', type=int, default=20, help='Number of denoising steps')
    parser.add_argument('--actual_steps', type=int, default=None, help='Number of denoising steps that are actually performed')
    parser.add_argument('--no_random', action='store_true', default=False, help='Use no randomness in reverse diffusion')
    parser.add_argument('--ancphore_path', type=str, default="../programs/", help='Path to folder with trained confidence model and hyperparameters')
    parser.add_argument('--no_final_step_noise', action='store_true', default=False, help='Use no noise in the final step of the reverse diffusion')
    parser.add_argument('--ode', action='store_true', default=False, help='Use ODE formulation for inference')
    parser.add_argument('--no_torsion', action='store_true', default=False, help='Use ODE formulation for inference')
    parser.add_argument('--cutoff', type=float, default=None, help='Keep the ligands with highest fitscore greater than cutoff.')
    parser.add_argument('--min_similarity', type=float, default=-1.0, help='Exclude the ligands whose max similarity is lower than the min similarity.')
    parser.add_argument('--report_results', type=str2bool, default=True, help='Report the aligning results.')
    parser.add_argument('--keep_update', type=str2bool, default=False, help='Whether to keep the trajetory when denoising the ligand poses.')
    parser.add_argument('--fitness', type=int, default=1, help='The fitness score to rank the predicted poses.')
    parser.add_argument('--target_fishing', type=str2bool, default=False, help='Whether to conduct target fishing task.')
    args = parser.parse_args()

    if args.target_fishing:
        args.fitness = 5
    return args


def read_input(phore_ligand_csv=None, phore=None, ligand=None):
    records = []
    if phore_ligand_csv is not None and os.path.exists(phore_ligand_csv):
        records = pd.read_csv(phore_ligand_csv).drop_duplicates().to_dict('records')
        # records = pd.read_csv(phore_ligand_csv, names=['phore', 'ligand_description']).to_dict('records')
    else:
        phore_list = []
        ligand_list = []
        if phore is not None and ligand is not None and os.path.exists(phore):
            if os.path.isdir(phore):
                phore_list = [os.path.join(phore, f) for f in os.listdir(phore)]
            elif os.path.isfile(phore):
                phore_list = [phore]
            if os.path.exists(ligand):
                if os.path.isdir(ligand):
                    ligand_list = [os.path.join(ligand, f) for f in os.listdir(ligand)]
                elif ligand.endswith('.smi'):
                    ligand_list = [line.strip() for line in open(ligand, 'r').readlines()]
                else:
                    ligand_list = [ligand]

        for p in phore_list:
            for l in ligand_list:
                records.append({'phore': p, 'ligand_description': l})
    if records:
        return records
    else:
        raise ValueError('Invalid input. Either phore_ligand_csv or protein and ligand must be specified')


def fit(args, model, complex_graphs, device, t_to_sigma, tmp_log="", n_report=1000):
    t_schedule = get_t_schedule(inference_steps=args.inference_steps)
    tr_schedule, rot_schedule, tor_schedule = t_schedule, t_schedule, t_schedule

    dataset = ListDataset(complex_graphs) if not args.save_single else ListFilenameDataset(complex_graphs)
    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)
    fitscore = []
    run_times = []
    names = []
    initial_poses = []
    dock_poses = []
    N = args.sample_per_complex if hasattr(args, 'sample_per_complex') else 1
    std_time = time.time()
    # for orig_complex_graph in tqdm(loader):
    for batch_idx, orig_complex_graph in enumerate(loader):
        name = orig_complex_graph.name[0]
        if args.min_similarity > 0:
            try:
                max_sim = get_perfect_similarity(orig_complex_graph[0])
                if max_sim < args.min_similarity:
                    print(f"[I] `{name}` is excluded due to pharmacophore fingerprint similarity constraints ({max_sim:.2f} < {args.min_similarity:.2f}).")
                    continue
            except Exception as e:
                print(f"[W] Failed to calculate the pharmacophore fingerprint similarity of `{name}`, skipped. {e}")


        docked_file = os.path.join(args.run_dir, f"ranked_poses/{name}_ranked.sdf")
        log_file = os.path.join(args.run_dir, f"mapping_process/{name}/{name}_dock.log")

        if (not os.path.exists(docked_file)) or (not os.path.exists(log_file)) or args.overwrite:
            data_list = [copy.deepcopy(orig_complex_graph) for _ in range(N)]
            randomize_position(data_list, args.no_torsion, False, args.tr_sigma_max, keep_update=getattr(args, 'keep_traj', False))
                
            if len(orig_complex_graph['ligand'].batch) == 0:
                print(f"[W] Graph {[orig_complex_graph['name']]} with 0 atoms, skipped")
                continue

            if getattr(args, 'keep_update', False):
                initial_poses.append([g['ligand'].pos.cpu().numpy() for g in data_list])
            
            predictions_list = None
            failed_convergence_counter = 0
            start_time = time.time()

            failed_flag = False
            while predictions_list == None:
                try:
                    predictions_list, _ = sampling_phore(data_list=data_list, 
                                                        model=model.module if device.type=='cuda' and hasattr(model, 'module') else model,
                                                        inference_steps=args.inference_steps,
                                                        tr_schedule=tr_schedule, rot_schedule=rot_schedule,
                                                        tor_schedule=tor_schedule,
                                                        device=device, t_to_sigma=t_to_sigma, model_args=args,
                                                        no_random=args.no_random, ode=args.ode, 
                                                        batch_size=args.batch_size,
                                                        no_final_step_noise=args.no_final_step_noise)
                except Exception as e:
                    if 'failed to converge' in str(e):
                        failed_convergence_counter += 1
                        if failed_convergence_counter > 5:
                            failed_flag = True
                            print('[W] SVD failed to converge 5 times - skipping the complex')
                            break
                        print('[W] SVD failed to converge - trying again with a new sample')
                    else:
                        print(f"[W] Error occured when fitting {name} to the reference pharamcophore, skipped. {e}")
                        failed_flag = True
                        break

            if not failed_flag and  predictions_list is not None:
                run_time = time.time() - start_time
                run_times.append(run_time)
                filterHs = torch.not_equal(orig_complex_graph['ligand'].x[:, 0], 0).cpu().numpy()
                ligand_pos = np.asarray(
                    [complex_graph['ligand'].pos.cpu().numpy()[filterHs] for complex_graph in predictions_list])
                
                phore_file = orig_complex_graph.phore_file[0]
                mol = Chem.RemoveAllHs(copy.deepcopy(orig_complex_graph.mol[0]))
                dock_pose = ligand_pos + orig_complex_graph.original_center.cpu().numpy()
                # print(f'phore_file: {phore_file}')
                scores = calculate_fitscore(args, dock_pose, name, mol, phore_file=phore_file, store_ranked_pose=True)
                if scores is None or len(scores) == 0:
                    fitscore.append([-2.0]*N)
                    print(f"[W] fitscore calculated with error and set as -2.0 for `{name}`")
                else:
                    fitscore.append(scores)
                names.append(name)

                json.dump(
                    {'name': name, 'fitscore': scores, 'run_time': run_time}, 
                    open(log_file, 'w'),
                    indent=4
                )
                if getattr(args, 'keep_update', False):
                    dock_poses.append([g.docked_poses for g in predictions_list])

        else:
            log = json.load(open(log_file, 'r'))
            names.append(log['name'])
            fitscore.append(log['fitscore'])
            run_times.append(log['run_time'])

        if (batch_idx + 1) % n_report == 0:
            print(f'[I] {batch_idx + 1}/{len(loader)} processed...')
            if tmp_log != "":
                tmp_dict = {'name': names, 'fitscore': fitscore, 'run_time': run_times, 
                            'batch': batch_idx, 'total_time': time.time() - std_time}
                json.dump(tmp_dict, open(tmp_log, 'w'), indent=4)

            
        if getattr(args, 'keep_update', False):
            metrics = {'name': names, 'fitscore': fitscore, 'run_time': run_times, 'initial_poses': initial_poses, 'dock_poses': dock_poses} 
        else:
            metrics = {'name': names, 'fitscore': fitscore, 'run_time': run_times} 

    
    return metrics


def get_perfect_similarity(g, weights=[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], 
                        alpha=[1.0, 1.0, 0.7, 1.0, 1.0, 0.7, 1.0, 1.0, 0.7, 1.0, 0.837],
                        debug=False):
    phore_volume = g['phore'].phoretype.sum(dim=0)
    phore_overlap = torch.min(g['ligand'].ph, phore_volume)
    coeff = torch.tensor(weights).float()
    if alpha is not None:
        # vol *= GCI2 * pow(PI/(p1.alpha + p2.alpha), 1.5);
        # vol *= exp(-(p1.alpha * p2.alpha) * r2/(p1.alpha + p2.alpha));
        volume_coeff = 7.999999999 * (torch.tensor(alpha) * torch.pi / 2) ** 1.5
        coeff *= volume_coeff
    
    weighted_volume = (phore_volume * coeff).sum()

    if debug: 
        print(f"\n[{g.name}] phore_volume: {phore_volume}")
        print(f"[{g.name}] ligand_fp: {g['ligand'].ph}")
        print(f"[{g.name}] phore_overlap: {phore_overlap}")

    if weighted_volume == 0:
        return -1.
    else:
        weighted_overlap = (phore_overlap * coeff).sum()
        if debug or weighted_overlap == 0: 
            if not debug:
                print(f"[{g.name}] phore_volume: {phore_volume}")
                print(f"[{g.name}] ligand_fp: {g['ligand'].ph}")
                print(f"[{g.name}] phore_overlap: {phore_overlap}")
            print(f"[{g.name}] weighted_overlap: {weighted_overlap}")
        similarity = weighted_overlap / weighted_volume
        return similarity.item()


def my_max(x):
    return max(x) if len(x) else -2.0


def analyze_results(args, results):
    df = pd.DataFrame(results)
    df['max_fitscore'] = df['fitscore'].map(my_max)
    df['top5_mean_fitscore'] = df['fitscore'].map(lambda x: np.sort(x)[-5:].mean().item())
    df['target'] = df['name'].map(lambda x: x.split('__')[0])
    df['ligand'] = df['name'].map(lambda x: x.split('__')[1])
    df = df.sort_values(by= ['max_fitscore', 'top5_mean_fitscore'], ascending=False)
    dump_file = os.path.join(args.out_dir, "ranked_results.csv")
    print(f"[I] Dumping results to `{dump_file}`")
    df = df[['target', 'ligand', 'name', 'run_time', 'max_fitscore', 'top5_mean_fitscore', 'fitscore']]
    df.to_csv(dump_file, sep='\t', index=False)

    if args.cutoff is not None:
        df[df['max_fitscore'] >= args.cutoff].to_csv(
            os.path.join(args.out_dir, f"ranked_results_gt{args.cutoff}.csv"), sep='\t', index=False)
    
    if args.report_results:
        print()
        print("#"*25 + " Pharmacophore Alignment Summary " + "#"*25)
        # show = ['target', 'ligand', 'max_fitscore', 'top5_mean_fitscore']
        num_ft_gt_0_7 = len(df[df['max_fitscore'] >= 0.7])
        num_ft_gt_0_4 = len(df[df['max_fitscore'] >= 0.4])
        print(f"Number of ligands with fitscore greater than 0.7: {num_ft_gt_0_7} ({100 * num_ft_gt_0_7/len(df):.2f}%)")
        print(f"Number of ligands with fitscore greater than 0.4: {num_ft_gt_0_4} ({100 * num_ft_gt_0_4/len(df):.2f}%)")
        print(f"Max fitscore: {df['max_fitscore'].max().item():.4f}")
        print(f"Average max fitscore: {df['max_fitscore'].mean().item():.4f}")
        print(f"Average runtime: {df['run_time'].mean().item():.4f}")


def welcome():
    print("*"*62)
    print("*"*2 + " "*24 + " DiffPhore" + " "*24 + "*"* 2)
    print("*"*2 + " "*8 + "Diffusion Model For Pharmacophre Alignment" + " "*8 + "*"* 2)
    print("*"*62)
    print(f"[{time.strftime('%Y/%m/%d-%H:%M:%S')}]")
    print(f"Current Working Dir: {os.getcwd()}")
    os.system("echo Current Hostname: $(hostname)")
    print(f'Current PID: {os.getpid()}')
    print(f"Current Command: {' '.join(sys.argv)}")
    if "CUDA_VISIBLE_DEVICES" in os.environ:
        print(f"Current GPU: {os.environ['CUDA_VISIBLE_DEVICES']}")


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    warnings.filterwarnings("ignore", category=UserWarning)
    args = parse_args()
    result_file = os.path.join(args.out_dir, 'inference_results.json')

    # set_seed(607201)
    with open(f'{args.model_dir}/model_parameters.yml') as f:
        score_model_args = Namespace(**yaml.full_load(f))

    score_model_args.sample_per_complex = args.sample_per_complex
    score_model_args.run_dir = args.out_dir
    score_model_args.inference_steps = args.inference_steps
    score_model_args.actual_steps = args.actual_steps
    score_model_args.ancphore_path = args.ancphore_path
    score_model_args.ode = args.ode
    score_model_args.no_torsion = args.no_torsion
    score_model_args.no_random = args.no_random
    score_model_args.no_final_step_noise = args.no_final_step_noise
    score_model_args.overwrite = args.overwrite
    score_model_args.min_similarity = args.min_similarity
    score_model_args.keep_update = args.keep_update
    score_model_args.fitness = args.fitness

    t_to_sigma = partial(t_to_sigma_compl, args=score_model_args)

    protein_ligand_records = read_input(args.phore_ligand_csv, args.phore, args.ligand)

    transform = NoiseTransformPhore(t_to_sigma=t_to_sigma, no_torsion=score_model_args.no_torsion, 
                                    tank_args={}, contrastive=True, 
                                    return_node= getattr(score_model_args, 'return_node', False),
                                    contrastive_model_dir=score_model_args.contrastive_model_dir) \
                                        if hasattr(score_model_args, 'contrastive') and score_model_args.contrastive else None
    inf_dataset = PhoreDataset(transform=transform, root='', cache_path=args.cache_path, split_path=args.split_file, 
                               protein_ligand_records=protein_ligand_records, dataset='VS',
                               num_workers=args.num_workers, keep_local_structures=args.keep_local_structures,
                               remove_hs=True, max_lig_size=None, save_single=False, use_sdf=True, near_phore=False, keep_original=True,
                               consider_ex=True, neighbor_cutoff=5.0, ex_connected=True, use_LAS_constrains=True, 
                               tank_features=False, use_phore_rule=True, matching=False, popsize=20, maxiter=20, require_ligand=True)
    score_model_args.save_single = inf_dataset.save_single

    print("[I] Number of fitting samples:", len(inf_dataset))
    if len(inf_dataset) < 1:
        print("[E] No valid fitting samples, please check your input. exit.")
        exit(0)
    # print("Generate Dataset Only, exit.")
    # exit(0)

    results = {}
    if not os.path.exists(result_file) or args.overwrite:
        if not os.path.exists(args.out_dir):
            os.makedirs(args.out_dir, exist_ok=True)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = get_model(score_model_args, device, t_to_sigma=t_to_sigma, no_parallel=True)

        print(f'[I] Loading state dict from `{args.model_dir}/{args.ckpt}`')
        state_dict = torch.load(f'{args.model_dir}/{args.ckpt}', map_location=torch.device('cpu')) 
        model.load_state_dict(state_dict, strict=True)
        model = model.to(device)
        model.eval()
        graphs = inf_dataset.graphs if not inf_dataset.save_single else inf_dataset.graph_pkls
        print("\n>> Starting to fit <<")
        print(f"[I] Please check the process files in `{os.path.join(args.out_dir, 'mapping_process/')}`")
        print(f"[I] Please check the ranked poses in `{os.path.join(args.out_dir, 'ranked_poses/')}`")
        results = fit(score_model_args, model, graphs, device, t_to_sigma, tmp_log=result_file+'.tmp')
        if os.path.exists(result_file+'.tmp'):
            shutil.move(result_file+'.tmp', result_file)

        if args.keep_update:
            pickle.dump(results, open(result_file+'.pkl', 'wb'))
        else:
            json.dump(results, open(result_file, 'w'), indent=4)

    else:
        results = json.load(open(result_file, 'r'))
    if results:
        analyze_results(args, results)


if __name__ == '__main__':
    st = time.time()
    welcome()
    main()
    end = time.time()
    print(f"Job Finished! {end-st:.3f} seconds cost.")
